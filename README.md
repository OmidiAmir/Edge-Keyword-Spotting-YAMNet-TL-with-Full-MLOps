# Keyword Spotting (Speech Commands) ‚Äî MLOps-Ready Project

[![CI](https://github.com/OmidiAmir/Edge-Keyword-Spotting-YAMNet-TL-with-Full-MLOps/actions/workflows/ci.yml/badge.svg)](https://github.com/OmidiAmir/Edge-Keyword-Spotting-YAMNet-TL-with-Full-MLOps/actions/workflows/ci.yml)
![Python](https://img.shields.io/badge/python-3.11+-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-ready-brightgreen)
![MLflow](https://img.shields.io/badge/MLflow-tracking-success)
![Docker](https://img.shields.io/badge/Docker-supported-informational)

Fast, lightweight keyword spotting on the **Google Speech Commands** dataset (subset of 10 common words).  
Trains quickly (<20 min on RTX 3060) and demonstrates a full **MLOps workflow** step by step:  
data ‚Üí model ‚Üí tests ‚Üí config ‚Üí MLflow tracking ‚Üí CI/CD ‚Üí FastAPI API.

---

## üöÄ Features so far
- **Dataset**: `KSDataset` with MFCC features + class filtering (10 keywords).
- **Training**: baseline CNN (~0.5M params) with YAML config.
- **Tracking**: MLflow (metrics, artifacts, models).
- **Export**: TorchScript (`models/ts_model.pt`).
- **Serving**: FastAPI app with robust audio decoding (ffmpeg + libsndfile).
- **CI**: GitHub Actions for tests (and easy extension to Docker).

---

```bash

keyword-spotting-mlops/
‚îú‚îÄ pyproject.toml
‚îú‚îÄ .gitignore
‚îú‚îÄ README.md
‚îú‚îÄ data/                   # dataset files (gitignored)
‚îÇ   ‚îî‚îÄ SpeechCommands
‚îÇ       ‚îî‚îÄ speech_commands_v0.02
‚îÇ            ‚îî‚îÄ ...
‚îú‚îÄ models/                 # exported TorchScript models
‚îú‚îÄ scripts/                # CLI entry points
‚îÇ    ‚îú‚îÄ download_data.py   # downloads Speech Commands
‚îÇ    ‚îú‚îÄ train_model.py     # training with MLflow
‚îÇ    ‚îú‚îÄ evaluate_model.py  # evaluation for specific run
‚îÇ    ‚îú‚îÄ export_model.py    # export TorchScript model
‚îÇ    ‚îî‚îÄ infer_wav.py       # inference on single WAV file
‚îú‚îÄ src/
‚îÇ   ‚îú‚îÄ kws/
‚îÇ   ‚îÇ   ‚îú‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ data/           # dataset utils
‚îÇ   ‚îÇ   ‚îÇ  ‚îî‚îÄ dataset.py
‚îÇ   ‚îÇ   ‚îú‚îÄ models/         # CNN model
‚îÇ   ‚îÇ   ‚îî‚îÄ training/       # training loop
‚îú‚îÄ tests/                  # pytest unit + smoke tests
‚îÇ   ‚îî‚îÄ test_data.py
‚îî‚îÄ .github/workflows/
    ‚îî‚îÄ ci.yml              # GitHub Actions workflow

```

---
## üõ† Requirements
- Python **3.11+**
- PyTorch, Torchaudio, NumPy, tqdm, **MLflow**, **FastAPI**, **Uvicorn**
---

## üõ† Installation (dev)
```bash
python -m venv .venv
# Activate venv
.venv\Scripts\activate   # Windows
source .venv/bin/activate # macOS/Linux

pip install -e .
```
---

## üìÇ Dataset
- **Source:** [Google Speech Commands](https://arxiv.org/abs/1804.03209) (v0.02)  
- **Selected classes:** yes, no, up, down, left, right, on, off, stop, go

- Sampling rate: **16 kHz**  
- Format: WAV, 1-second duration

```bash
python scripts/download_data.py --out data/SpeechCommands

```
---

## üß† Baseline Model

- Features: MFCC (n_mfcc=40) + standardization
- Architecture: 3 convolutional blocks ‚Üí Global Average Pool ‚Üí Fully Connected
- Parameters: ~0.5M
- Training time: < 20 minutes on RTX 3060

--- 

## üß™ Train ‚Üí Evaluate ‚Üí Export
```bash
# Train (logs to MLflow)
python scripts/train_model.py --config configs/train_model.yaml

# Evaluate a specific run
python scripts/evaluate_model.py --run_id <RUN_ID>

# Export TorchScript
python scripts/export_model.py --run_id <RUN_ID> --out models/ts_model.pt

# Local single-file inference (sanity)
python scripts/infer_wav.py --wav data/SpeechCommands/speech_commands_v0.02/yes/0a7c2a8d_nohash_0.wav

```

---

## üìà MLflow Tracking
```bash
mlflow ui
```
Open http://127.0.0.1:5000 to view experiments, metrics, and checkpoints.

---

## üåê Serving with FastAPI

Once you‚Äôve exported a TorchScript model to `models/ts_model.pt`, you can serve it via FastAPI.

### Run locally (no Docker)
```bash
uvicorn kws.serve.app:app --host 0.0.0.0 --port 8000 --reload

```
Endpoints:
- GET /health ‚Üí service status
- GET /selftest ‚Üí sanity check (generates + reads back a dummy tone)
- POST /predict ‚Üí keyword prediction for uploaded .wav file
- POST /predict-bytes ‚Üí keyword prediction from raw audio bytes

Example request:
```bash
# Linux/macOS
curl -F "file=@data/SpeechCommands/.../yes/0a7c2a8d_nohash_0.wav" http://localhost:8000/predict

# Windows PowerShell
$f="C:\path\to\some.wav"
curl.exe -F "file=@$f" http://localhost:8000/predict
```
---
## üì¶ Docker Support
Build & run containerized API:
```bash
# Build
docker build -t kws-api .

# Run
docker run --rm -p 8000:8000 \
  -v "${PWD}/models:/app/models:ro" \
  -e MODEL_PATH=/app/models/ts_model.pt \
  kws-api
```
test
```bash
curl http://localhost:8000/health
curl -F "file=@data/SpeechCommands/.../yes/0a7c2a8d_nohash_0.wav" http://localhost:8000/predict
```

---

## üîÑ CI/CD with GitHub Actions

This repo includes a GitHub Actions workflow (.github/workflows/ci.yml) that runs automatically on every push or pull request:
- ‚úÖ Unit tests (dataset + preprocessing)
- ‚úÖ Smoke tests (training & inference run)
- üîú Extend with Docker build/push

This ensures code stays stable and reproducible as new MLOps components are added.

---
